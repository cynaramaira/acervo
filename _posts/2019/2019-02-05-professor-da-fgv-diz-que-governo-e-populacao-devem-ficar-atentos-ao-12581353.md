---
id: 12581353
date: "2019-02-05T18:00:30Z"
last_modified_at: null
tags:
  - "governo"
  - "populacao"
  - "professor"
categories:
  - "noticias"
title: "Professor da FGV diz que governo e popula\u00e7\u00e3o devem ficar atentos ao mau uso da IA"
sutia: null
chapeu: null
autor: null
imagem: "https://imagens.ne10.uol.com.br/legado/blogsne10/jamildo/uploads/2019/01/15jan2018-parlamentares-do-psl-fazem-escala-durante-viagem-a-china-nesta-terca-feira-15-1547572596190_v2_900x506.jpg"
---
{% raw %}
<p><p>Uma semana antes da posse dos novos deputados federais, alguns eleitos pelo PSL divulgaram com estardalhaço que estariam indo visitar a China, para conhecer programas de reconhecimento facial que poderiam ser usados em aplicações no Brasil, segundo justificaram. O filósofo Olavo de Carvalho deu uma espinafrada nos aliados bolsonaristas falando que eles estavam querendo entregar o ouro aos inimigos, sem entender o alcance das novas tecnologias.</p></p>
<p><p>Pois bem...</p></p>
<p><p>O coordenador do MBA de Marketing Digital da Fundação Getulio Vargas (FGV), André Miceli, pondera que apesar dos benefícios da Inteligência Artificial (IA), o mau uso desta tecnologia pode produzir efeitos nocivos sobre a sociedade e, por isso, a população e seus governantes devem estar atentos às finalidades de aplicação da IA.</p></p>
<p><p>"Um algoritmo já pode escolher o que vamos ler na internet, nos manipular politicamente e aumentar ainda mais a discriminação na sociedade, além de julgar quem tem mais chances de viver ou morrer. Avaliar os riscos é fundamental para combater impactos que podem ser extremamente perigosos. Portanto, é essencial trabalhar para minimizar esses impactos negativos de iniciativas que podem facilitar o nosso dia a dia", ressalta André Miceli.</p></p>
<p><p>O especialista em Tecnologia da Informação cita como exemplo a China, país líder na tecnologia de reconhecimento facial. Miceli afirma que milhões de pessoas já foram mapeadas, sendo foco de alertas disparados para instituições governamentais e particulares. "Esse tipo de informação pode ser usado por governos autoritários para criar alguma imposição de comportamento. Um exemplo simples é que alguns trens públicos do país asiático já não abrem suas portas quando alguém reconhecido como devedor tentar entrar. Isso seria mais ou menos um SPC Chinês", destaca o professor da FGV.</p></p>
<p><p>André Miceli ressalta que esses algoritmos, além de analisar créditos bancários, podem estabelecer quem pode ou não comprar um imóvel ou entrar em um clube social. "Neste caso, eles podem entregar pontuações de efeito preconceituoso, como, por exemplo, o caso do chatbot da Microsoft que se transformou em uma ferramenta racista e foi desligado pela empresa", cita o especialista.</p></p>
<p><p><strong>Risco à democracia</strong></p></p>
<p><p>André Miceli lembra que algoritmos já escolhem que tipo de informações temos acesso nas redes sociais e que chatbots maliciosos podem ser criados para discutir política com a possibilidade de recorrer a dados mais rápido que os seres humanos. "Essa customização pode trazer riscos à democracia porque pode potencializar e beneficiar um determinado candidato. As eleições deste ano na África do Sul e na Nigéria estão propensas a sofrer interferências desse tipo de tecnologia", explica o professor da FGV.</p></p>
<p><p>Miceli pondera que até vídeos e áudios já podem ser manipulados. Segundo ele, já é possível criar escândalos e falsas acusações. "Essa prática pode gerar alertas e notícias falsas especificamente para um grupo de pessoas em busca de benefícios de algum candidato. Aconteceu um fato na índia marcante no ano passado. Pessoas foram linchadas e mortas por falsas acusações de pedofilia", descreve o especialista.</p></p>
<p><p><strong>Vida ou morte</strong></p></p>
<p><p>O professor da FGV acrescenta que a Inteligência Artificial já reconhece quem tem doença transmissível. Portanto, a tecnologia pode controlar epidemias e, ao mesmo tempo, excluir pessoas que são um risco para a sociedade. "Outro uso é em situações de guerra. Os Estados Unidos têm drones que avaliam percursos com imagens cedidas pelo Google. Qualquer erro pode ser fatal para a morte de um inocente", avalia André Miceli.</p></p>
<p><p>Por fim, o professor da FGV recorda o caso do carro autônomo de um aplicativo de mobilidade que atropelou e matou uma mulher apesar da falha humana. Segundo Miceli, essa tecnologia também pode ser fatal, portanto precisa de aperfeiçoamento. "Em uma situação adversa e hipotética, o algoritmo poderá ter que escolher em atropelar um jovem ou idoso. Quem ele vai escolher? Na Índia, uma vaca é sagrada e no resto do mundo? Essa tecnologia já é uma realidade e no Brasil deve começar pelo agronegócio."</p></p>
<p><p><strong>Papel da sociedade</strong></p></p>
<p><p>André Miceli diz que ao longo dos anos, vimos diversas tecnologias transformarem a história da humanidade, no entanto, ela deve ser usada para promover avanços e desenvolvimento para os seres humanos. "Para que as coisas não saiam do controle, é imprescindível que pesquisadores e cientistas eduquem a sociedade quanto ao uso responsável da Inteligência Artificial. Ao fazer isso, não vamos colocar a vida humana em risco e aproveitaremos todos os pontos positivos que esses avançados tecnológicos podem nos proporcionar", propõe o professor da FGV.</p></p>
{% endraw %}